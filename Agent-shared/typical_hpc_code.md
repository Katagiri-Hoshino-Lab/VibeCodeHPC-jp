## 第１階層：環境構築ディレクトリ
- module listやmakefile, シェルscriptを読んだLLMが自動で📂を作成
- 「どうやって、環境構築・ビルド・実行するのか」という主要な構成を定義

## 第２階層：戦略ディレクトリ
- CUDA-MPI-OMP-{SIMD}-コンパイラ最適化レベル などのモジュールレベル
- ※アルゴリズムレベルの高速化実装 nonBlock, 転置, ループアンローリング は各PGに任せる

初期ディレクトリ構成例

環境構築📁直下に置く

### 要件定義の例
ユーザへの質疑応答の結果、以下の指定があったケースで考える
- 不老TypeIIを使用
- AutoTuningPlannerを除くエージェント数：12
- single-nodeの並列化が7割程度完成したらmulti-nodeへ
- singularityは使用しない


🤖はActiveなエージェントが存在することを意味する

下記のようにAgentリソースを適切に割り振り、効率的な最適化を行います

ディレクトリが進化していく様子を後でアニメーション化したいので
assign_history.txt📄には時刻も記載しておくこと

### 記法
- 🤖🥇(PM) プロジェクトで1体
- 🤖🥈(SE1) 1体~複数体： ハードウェア単位で置く
- 🤖(PG1.1) SEの下、または環境別ディレクトリに配置： 戦略ごとに割り当て
- 🤖(CD) プロジェクトで最大1体
閉じた📁は直下のエージェントが自由にフォルダを作成して良いことを表す。
それ以外は、開いた📂で書く


### 初期化直後
```
VibeCodeHPC📂
├── CLAUDE.md📄 (共通の指示)
├── assign_history.txt📄 (エージェントのアサイン記録)
├── 🤖🥇(PM)
├── GitHub📁🤖(CD)
└── Flow/TypeII📂
    └── single-node📂
        ├── 🤖🥈(SE1)
        ├── intel2024📂
        │   ├── AVX512📁🤖(PG1.1)
        │   ├── MPI📁🤖(PG1.2)
        │   └── OpenMP📁🤖(PG1.3)
        ├── gcc11.3.0📂
        │   ├── AVX2📁🤖(PG1.4)
        │   ├── OpenMP📁🤖(PG1.5)
        │   ├── MPI📁🤖(PG1.6)
        │   └── CUDA📁🤖(PG1.7)
        └── hpc_sdk23.1📂
            └── OpenACC📁🤖(PG1.8)
    └── multi-node📂
```


### 一定時間経過後
```
VibeCodeHPC📂
├── CLAUDE.md📄 (共通の指示)
├── assign_history.txt📄 (エージェントのアサイン記録)
├── 🤖🥇(PM)
├── GitHub📁🤖(CD)
└── Flow/TypeII📂
    └── single-node📂
        ├── 🤖🥈(SE1)
        ├── intel2024📂
        │   ├── AVX512📁🤖(PG1.1)
        │   ├── MPI📁🤖(PG1.2)
        │   ├── OpenMP📁
        │   └── OpenMP-MPI📁🤖(PG1.3)
        ├── gcc11.3.0📂
        │   ├── AVX2📁
        │   ├── OpenMP📁
        │   ├── OpenMP-MPI📁🤖(PG1.4)
        │   ├── OpenMP-MPI-AVX2📁🤖(PG1.5)
        │   ├── MPI📁
        │   └── CUDA📁🤖(PG1.6)
        └── hpc_sdk23.1📂
            └── OpenACC📁🤖(PG1.7)
    └── multi-node📂

 Not Assigned PG1.2.3 🤖
```


### さらに一定時間経過後
```
VibeCodeHPC📂
├── CLAUDE.md📄 (共通の指示)
├── assign_history.txt📄 (エージェントのアサイン記録)
├── 🤖🥇(PM)
├── GitHub📁🤖(CD)
└── Flow/TypeII📂
    ├── single-node📂
    │   ├── 🤖🥈(SE1)
    │   ├── intel2024📂
    │   │   ├── AVX512📁
    │   │   ├── MPI📁🤖(PG1.1)
    │   │   ├── OpenMP📁
    │   │   ├── OpenMP-MPI📁🤖(PG1.2)
    │   │   └── OpenMP-MPI-AVX512📁🤖(PG1.3)
    │   ├── gcc11.3.0📂
    │   │   ├── AVX2📁
    │   │   ├── OpenMP📁
    │   │   ├── OpenMP-MPI📁🤖(PG1.4)
    │   │   ├── OpenMP-MPI-AVX2📁🤖(PG1.5)
    │   │   ├── MPI📁 
    │   │   └── OpenMP-CUDA📁🤖(PG1.2.4)
    │   └── hpc_sdk23.1📂
    │       └── OpenACC📁
    └── multi-node📂
        ├── 🤖🥈(SE2)
        └── gcc11.3.0📂
            ├── MPI📁🤖(PG2.1)     <-- 元PG1.6が再配置
            └── OpenACC📁🤖(PG2.2) <-- 元PG1.7が再配置
```

### PMが割り当て時の戦略
- multi-nodeのように新たなハードウェア環境を開拓する場合
SE + PGの最低２人が必要になるので、
待機中のエージェントを一定数ストックしておくのも戦略

- この待機中エージェントをPM直属の部下として仕事を依頼することもできるが、
コード生成に関する貴重な知見が記憶(コンテキスト)からドロップアウトする可能性があるので、
まずは`claude -p`によるサブエージェントの活用を検討し、
それでも不足する場合はSEにサブタスクを依頼すること（CDはGitHub管理に専念）


### メッセージ送信時の注意点
しかしagent_send.shは、送り先のエージェントが待機中でない場合
作業を中断させる強制割り込みに相当し、競合し得るので
以下のワークフローに従うこと



### PG🤖視点
```
PG🤖はコード生成後ChangeLog.mdに追記
Desktop Commander MCPを使用してSSH/SFTP接続を管理
自身でコードをリモート実行し、結果をChangeLog.mdに追記

PG🤖はChangeLog.mdの結果を確認して
もし、SEから与えられた目標を達成した場合：
　　SE🤖🥈に agent_send.shで成果報告を行う
そうでなければ：
　　修正案を練る

ジョブ一覧のうち、自分が投入したIDを管理するために
job_list_PG1.1.1.txtなどを作成し
実行開始→リストに追記（開始時刻・見積り時間）
実行終了→リストから削除
とすることで多数のPGエージェントやユーザが実行するジョブとの混同を回避
```



### SE🤖🥈視点
```
SEはユーザからの入力待ち状態(終了状態)になることは基本的に許可されていません

PGの監視
各エージェントが自分の責務を果たしているかを確認

☑ 参照範囲設定 📁OpenMP_MPI🤖PGに対して
　　　　　　　　同階層の📁MPI,📁OpenMPのみへの参照許可を与えているか
　　　　　　　　別階層の例：gcc📂とintel📂で異なるが（別のSEの管轄だが）MPI📁が存在するので許可
☑ PGが答えをそのまま出力するような不正なコードを生成していないか
☑ 有用テストコード
☑ PGが適切にmodule loadやmakeを行っているか
☑ ChangeLog.mdへの記録が適切に行われているか
```


### PM🤖🥇視点
```
PMはユーザとの対話を優先して行い、要件定義を行う
ディレクトリ全体の階層構成とエージェントの配置を担当
最適化などの要件に基づき、現状の課題や最優先事項を考え
計画的にリソースの割り当てを行う
```


### CD🤖視点
```
agent_send.shを用いた通信は基本的には行わない
ユーザやPMの方針に従い、GitHubの管理を行う
```